---
title: "【実装編】3D CADデータをAIにぶち込め！Pythonで始める無料サロゲートモデル構築"
emoji: "🐍"
type: "tech"
topics: ["3DCAD", "CAE", "Pytorch", "Python", "DeepLearning"]
published: true
---

## はじめに：3D形状を「数値」として扱う魔法

前回、3D CADデータ（STL/STEP）を使ったサロゲートモデルの考え方について触れました。
今回は**「じゃあ、具体的にどう書くの？」**という疑問に答えるべく、最小構成の実装コードを公開します。

高価なソフトウェアは一切不要。`numpy-stl` と `PyTorch` さえあれば、あなたのPCで3D形状を理解するAIが動き出します。

---

## 1. 3DデータをどうやってAIに食わせるか？

AIは「STLファイル」をそのままでは読み込めません。そこで、メッシュ（三角形の集まり）を**「点群（Point Cloud）」**に変換するのが最もシンプルで強力な方法です。



提供いただいた以下のコードでは、STLの各三角形の「重心（Centroid）」を抽出することで、形状の特徴を捉えた点群を生成しています。

```python:main.py
import numpy as np
from stl import mesh
import torch
import torch.nn as nn
import torch.optim as optim

# 1. STLデータの読み込みとサンプリング
def load_stl_as_point_cloud(file_path, num_samples=1024):
    # STLファイルのロード
    your_mesh = mesh.Mesh.from_file(file_path)
    
    # 各三角形の重心を点として抽出（簡易的な点群化）
    points = your_mesh.centroids
    
    # 指定したサンプル数に調整（不足ならランダム選択、過剰なら間引き）
    if len(points) > num_samples:
        idx = np.random.choice(len(points), num_samples, replace=False)
        points = points[idx]
    
    return points.astype(np.float32)
```
ここがプロのポイント： CAEのメッシュは場所によって細かさが違います。重心を取ることで、形状が複雑な（＝メッシュが細かい）部位ほど点密度が高くなり、AIが「ここが大事な場所なんだな」と理解しやすくなります。

2. モデルの構築：PointNetライクなサロゲートモデル
次に、抽出した点群から「最大応力」や「圧力損失」を予測する脳みそ（ニューラルネットワーク）を作ります。

```Python
# 2. 簡易サロゲートモデルの定義 (PointNetライクな構造)
class SimpleSurrogateModel(nn.Module):
    def __init__(self, num_points):
        super(SimpleSurrogateModel, self).__init__()
        # 座標(x, y, z)のフラットな入力を特徴量へ変換
        self.fc1 = nn.Linear(num_points * 3, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 1)  # 出力：最大応力や最大変位など
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(x.size(0), -1) # (Batch, Points, 3) -> (Batch, Points*3)
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        return self.fc3(x)
```
このモデルは、数千個の点の座標を一度に見て、「このへんがボコっとしているから、応力が高そうだ」という傾向を学習します。

3. 学習プロセス：物理とAIの融合
最後に、CAEの解析結果（教師データ）を使って学習を回します。

```Python
# 3. 学習プロセスの例
num_samples = 1024
model = SimpleSurrogateModel(num_samples)
criterion = nn.MSELoss() # 二乗誤差で最適化
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 仮の入力データ(8個分)と正解ラベル（CAE解析結果）
dummy_input = torch.randn(8, num_samples, 3) 
dummy_label = torch.randn(8, 1)

# 学習ステップ
optimizer.zero_grad()
output = model(dummy_input)
loss = criterion(output, dummy_label)
loss.backward()
optimizer.step()

print(f"Training Loss: {loss.item()}")
```
精度を上げるための「データ拡張」3箇条
コードが書けたら、次にやるべきは**「データの質」**を上げることです。前回の記事でも紹介した通り、3Dデータ特有のテクニックをここに盛り込みましょう。

座標の正規化: points -= np.mean(points, axis=0) で重心を原点に飛ばしましょう。これだけで学習が爆速になります。

回転の追加: numpy-stl の機能を使って、モデルを少しずつ回転させたデータを増やして学習させましょう（データ拡張）。

ノイズ付加: 点の座標に np.random.normal で微小なノイズを加えると、メッシュの粗さに左右されないタフなモデルになります。

## まとめ：エンジニアよ、コードを書こう
「AIは専門家がやるもの」という時代は終わりました。 今回紹介したように、numpy-stl で形状を触り、PyTorch で予測する。これだけで、数千回のシミュレーションを待つ時間は「数ミリ秒の推論」に変わります。

まずは手元のSTLファイルを10個集めて、このコードを動かすところから始めてみませんか？